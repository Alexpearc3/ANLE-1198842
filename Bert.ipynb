{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:23.942793Z",
     "end_time": "2023-05-05T03:10:24.500150Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      label                                  tagged_in_context\n",
      "0            not_propaganda         No, <BOS> he <EOS> will not be confirmed. \n",
      "1            not_propaganda  This declassification effort <BOS> won’t make ...\n",
      "2               flag_waving  The Obama administration misled the <BOS> Amer...\n",
      "3            not_propaganda  “It looks like we’re capturing the demise of t...\n",
      "4            not_propaganda           <BOS> Location: Westerville, Ohio <EOS> \n",
      "...                     ...                                                ...\n",
      "2409         not_propaganda  <BOS> We support and appreciate <EOS> your bus...\n",
      "2410         not_propaganda  International Atomic Energy Agency (IAEA) Dire...\n",
      "2411         not_propaganda  What has been done: there has been work on for...\n",
      "2412         not_propaganda  This is <BOS> the law of gradualness not the g...\n",
      "2413  name_calling,labeling  In it, Jews are described as: “arrogant,” “jea...\n",
      "\n",
      "[2414 rows x 2 columns]\n",
      "                      label                                  tagged_in_context\n",
      "0            not_propaganda         No, <BOS> he <EOS> will not be confirmed. \n",
      "1            not_propaganda  This declassification effort <BOS> won’t make ...\n",
      "2               flag_waving  The Obama administration misled the <BOS> Amer...\n",
      "3            not_propaganda  “It looks like we’re capturing the demise of t...\n",
      "4            not_propaganda           <BOS> Location: Westerville, Ohio <EOS> \n",
      "...                     ...                                                ...\n",
      "2409         not_propaganda  <BOS> We support and appreciate <EOS> your bus...\n",
      "2410         not_propaganda  International Atomic Energy Agency (IAEA) Dire...\n",
      "2411         not_propaganda  What has been done: there has been work on for...\n",
      "2412         not_propaganda  This is <BOS> the law of gradualness not the g...\n",
      "2413  name_calling,labeling  In it, Jews are described as: “arrogant,” “jea...\n",
      "\n",
      "[2414 rows x 2 columns]\n",
      "                                      tagged_in_context                  label\n",
      "0            No, <BOS> he <EOS> will not be confirmed.          not_propaganda\n",
      "1     This declassification effort <BOS> won’t make ...         not_propaganda\n",
      "2     The Obama administration misled the <BOS> Amer...            flag_waving\n",
      "3     “It looks like we’re capturing the demise of t...         not_propaganda\n",
      "4              <BOS> Location: Westerville, Ohio <EOS>          not_propaganda\n",
      "...                                                 ...                    ...\n",
      "2409  <BOS> We support and appreciate <EOS> your bus...         not_propaganda\n",
      "2410  International Atomic Energy Agency (IAEA) Dire...         not_propaganda\n",
      "2411  What has been done: there has been work on for...         not_propaganda\n",
      "2412  This is <BOS> the law of gradualness not the g...         not_propaganda\n",
      "2413  In it, Jews are described as: “arrogant,” “jea...  name_calling,labeling\n",
      "\n",
      "[2414 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                   tagged_in_context           label\n0         No, <BOS> he <EOS> will not be confirmed.   not_propaganda\n1  This declassification effort <BOS> won’t make ...  not_propaganda\n2  The Obama administration misled the <BOS> Amer...     flag_waving\n3  “It looks like we’re capturing the demise of t...  not_propaganda\n4           <BOS> Location: Westerville, Ohio <EOS>   not_propaganda",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tagged_in_context</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>No, &lt;BOS&gt; he &lt;EOS&gt; will not be confirmed.</td>\n      <td>not_propaganda</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This declassification effort &lt;BOS&gt; won’t make ...</td>\n      <td>not_propaganda</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Obama administration misled the &lt;BOS&gt; Amer...</td>\n      <td>flag_waving</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>“It looks like we’re capturing the demise of t...</td>\n      <td>not_propaganda</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;BOS&gt; Location: Westerville, Ohio &lt;EOS&gt;</td>\n      <td>not_propaganda</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "train_data=pd.read_csv('propaganda_train.tsv',sep='\\t')\n",
    "test_data=pd.read_csv('propaganda_val.tsv',sep='\\t')\n",
    "print(train_data)\n",
    "print(train_data)\n",
    "training_df=pd.DataFrame(train_data,columns=[\"tagged_in_context\",\"label\"])\n",
    "testing_df=pd.DataFrame(test_data,columns=[\"tagged_in_context\",\"label\"])\n",
    "\n",
    "print(training_df)\n",
    "labellist=sorted(list(set(training_df['label'].unique()).union(set(testing_df['label'].unique()))))\n",
    "\n",
    "labels={label:i for i,label in enumerate(labellist)}\n",
    "labels\n",
    "reverse_index={value:key for (key,value)in labels.items()}\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:24.502659Z",
     "end_time": "2023-05-05T03:10:27.388757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'no',\n ',',\n '<',\n 'bo',\n '##s',\n '>',\n 'he',\n '<',\n 'e',\n '##os',\n '>',\n 'will',\n 'not',\n 'be',\n 'confirmed',\n '.',\n '[SEP]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,df,column='tagged_in_context'):\n",
    "        self.labels=[labels[label] for label in df['label']]\n",
    "        self.texts=[tokenizer(text.lower(),padding='max_length',max_length=512,truncation=True,return_tensors=\"pt\") for text in df[column]]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self,idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self,idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        batch_texts=self.get_batch_texts(idx)\n",
    "        batch_y=self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts,batch_y\n",
    "\n",
    "\n",
    "train_data=Dataset(training_df)\n",
    "test_data=Dataset(testing_df)\n",
    "train_data[0]\n",
    "train_data.texts[0]\n",
    "my_input = train_data.texts[0].input_ids\n",
    "my_input\n",
    "my_tokens=tokenizer.convert_ids_to_tokens(my_input[0])\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:27.390262Z",
     "end_time": "2023-05-05T03:10:27.433380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU acceleration NOT enabled.  If using Colab, have you changed the runtype type and selected GPU as the hardware accelerator?\n",
      "Using device: cpu\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda=torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "  print(\"GPU acceleration enabled\")\n",
    "else:\n",
    "  print(\"GPU acceleration NOT enabled.  If using Colab, have you changed the runtype type and selected GPU as the hardware accelerator?\")\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "# get number of GPUs available\n",
    "torch.cuda.device_count() # returns 1 in my case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:27.406286Z",
     "end_time": "2023-05-05T03:10:27.448933Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_inputs(input1,label,device):\n",
    "  label=label.to(device)\n",
    "  mask=input1['attention_mask'].to(device)\n",
    "  input_id=input1['input_ids'].squeeze(1).to(device)\n",
    "  return (input_id,mask,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:27.421338Z",
     "end_time": "2023-05-05T03:10:31.319869Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2009, 2001,  ...,    0,    0,    0],\n",
      "        [ 101, 2002, 3728,  ...,    0,    0,    0]]) tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]) tensor([7, 7], dtype=torch.int32)\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-0.7184, -0.4232, -0.9672,  ..., -0.8875, -0.6072,  0.5026],\n        [-0.9450, -0.6086, -0.9665,  ..., -0.8290, -0.6635,  0.8852]],\n       grad_fn=<TanhBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "train_dataloader=torch.utils.data.DataLoader(train_data,batch_size=2,shuffle=True)\n",
    "bert=BertModel.from_pretrained('bert-base-uncased')\n",
    "for train_input,train_label in train_dataloader:\n",
    "    input_id,mask,label=prepare_inputs(train_input,train_label,device)\n",
    "    output=bert(input_ids=input_id,attention_mask=mask,return_dict=False)\n",
    "    break\n",
    "\n",
    "print(input_id,mask,label)\n",
    "print(len(output))\n",
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:31.315856Z",
     "end_time": "2023-05-05T03:10:31.331911Z"
    }
   },
   "outputs": [],
   "source": [
    "#now we need to put a simple classification layer on top of BERT\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,dropout=0.5,num_classes=2):\n",
    "\n",
    "        super(BertClassifier,self).__init__()\n",
    "\n",
    "        self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear=nn.Linear(768,num_classes)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,input_id,mask):\n",
    "\n",
    "        last_hidden_layer,pooled_output = self.bert(input_ids=input_id,attention_mask=mask,return_dict=False)\n",
    "        dropout_output=self.dropout(pooled_output)\n",
    "        linear_output=self.linear(dropout_output)\n",
    "        final_layer=self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:31.336426Z",
     "end_time": "2023-05-05T03:10:31.359505Z"
    }
   },
   "outputs": [],
   "source": [
    "#we now need a training loop\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_data,val_data,learning_rate,epochs):\n",
    "\n",
    "    train_dataloader=torch.utils.data.DataLoader(train_data,batch_size=2,shuffle=True)\n",
    "    val_dataloader=torch.utils.data.DataLoader(test_data,batch_size=2)\n",
    "\n",
    "    use_cuda=torch.cuda.is_available()\n",
    "    device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "        criterion=criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train=0\n",
    "        total_loss_train=0\n",
    "        model.train()\n",
    "        for train_input,train_label in tqdm(train_dataloader):\n",
    "\n",
    "            input_id,mask, train_label=prepare_inputs(train_input,train_label,device)\n",
    "\n",
    "            output=model(input_id,mask)\n",
    "\n",
    "            batch_loss=criterion(output,train_label.long())\n",
    "            total_loss_train +=batch_loss.item()\n",
    "\n",
    "            acc=(output.argmax(dim=1)==train_label).sum().item()\n",
    "            total_acc_train+=acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val=0\n",
    "        total_loss_val=0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_input,val_label in val_dataloader:\n",
    "\n",
    "                input_id,mask, val_label=prepare_inputs(val_input,val_label,device)\n",
    "\n",
    "                output=model(input_id,mask)\n",
    "\n",
    "                batch_loss=criterion(output,val_label.long())\n",
    "\n",
    "                total_loss_val+=batch_loss.item()\n",
    "\n",
    "                acc=(output.argmax(dim=1)==val_label).sum().item()\n",
    "                total_acc_val+=acc\n",
    "\n",
    "        print(f'Epochs: {epoch_num+1} | Train Loss: {total_loss_train / len(train_data):.3f} | Train Accuracy: {total_acc_train/len(train_data):.3f}')\n",
    "        print(f'Val loss: {total_loss_val/len(val_data):.3f} | Val Accuracy: {total_acc_val / len(val_data):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T03:10:31.348969Z",
     "end_time": "2023-05-05T03:10:32.531419Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "model=BertClassifier(num_classes=len(labels.keys()))\n",
    "LR=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T02:17:14.896656Z",
     "start_time": "2023-05-05T00:13:42.463726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 362/1207 [34:50<1:21:20,  5.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43mLR\u001B[49m\u001B[43m,\u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[13], line 40\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_data, val_data, learning_rate, epochs)\u001B[0m\n\u001B[0;32m     37\u001B[0m     total_acc_train\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39macc\n\u001B[0;32m     39\u001B[0m     model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 40\u001B[0m     \u001B[43mbatch_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     43\u001B[0m total_acc_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPUNLE\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GPUNLE\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(model,train_data,test_data,LR,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir=\"bert-base-uncased-bookclassifier\"\n",
    "torch.save(model,output_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_dir=\"bert-base-uncased-bookclassifier\"\n",
    "complete_model=torch.load(input_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batchsize=2\n",
    "def evaluate(model,test_dataset):\n",
    "    model.eval()\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_dataset,batch_size=batchsize)\n",
    "\n",
    "    use_cuda=torch.cuda.is_available()\n",
    "    device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "\n",
    "    total_acc_test=0\n",
    "    with torch.no_grad():\n",
    "        count=0\n",
    "        predictions=[]\n",
    "        for test_input,test_label in tqdm(test_dataloader):\n",
    "            count+=batchsize\n",
    "            test_label=test_label.to(device)\n",
    "            mask=test_input['attention_mask'].to(device)\n",
    "            input_id=test_input['input_ids'].squeeze(1).to(device)\n",
    "            output=model(input_id,mask)\n",
    "            #print(output.argmax(dim=1),test_label)\n",
    "            predictions.append(output.argmax(dim=1))  #save the prediction for further analysis\n",
    "            acc=(output.argmax(dim=1)==test_label).sum().item()\n",
    "\n",
    "            total_acc_test+=acc\n",
    "            if count%100==0:\n",
    "                print(f'Accuracy so far = {total_acc_test/count: .3f}')\n",
    "\n",
    "    print(f'Test accuracy: {total_acc_test/len(test_dataset): .3f}')\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions=evaluate(model, test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flattened=[]\n",
    "for batch in predictions:\n",
    "    for pred in batch:\n",
    "        flattened.append(reverse_index[pred.item()])\n",
    "testing_df['prediction']=flattened\n",
    "testing_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_labels=testing_df['label']\n",
    "all_predictions=testing_df['prediction']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tp={}\n",
    "fp={}\n",
    "fn={}\n",
    "tn={}\n",
    "\n",
    "for label1,pred1 in zip(all_labels,all_predictions):\n",
    "    for label in labels.keys():\n",
    "        if label1==label:\n",
    "            if pred1==label:\n",
    "                tp[label]=tp.get(label,0)+1\n",
    "            else:\n",
    "                fn[label]=fn.get(label,0)+1\n",
    "\n",
    "        else:\n",
    "            if pred1==label:\n",
    "                fp[label]=fp.get(label,0)+1\n",
    "            else:\n",
    "                tn[label]=tn.get(label,0)+1\n",
    "\n",
    "\n",
    "\n",
    "precision={label:value/(value+fp.get(label,0)) for label,value in tp.items()}\n",
    "recall={label:value/(value+fn.get(label,0)) for label,value in tp.items()}\n",
    "f1={label:(2*value*recall.get(label,0))/(value+recall.get(label,0)) for label,value in precision.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision\n",
    "recall\n",
    "f1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testing_df.loc[47, \"text\"]\n",
    "len(testing_df.loc[47, \"text\"])\n",
    "len(testing_df.loc[1, \"text\"])\n",
    "texts = testing_df[\"text\"]\n",
    "lengths = [len(text) for text in texts]\n",
    "print(np.mean(lengths))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
